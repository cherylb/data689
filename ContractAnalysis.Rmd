---
title: "Contract Execution"
author: "Cheryl Bowersox"
date: "October 21, 2018"
output: word_document
---
## Introduction

As part of a large IT organization one of my team's responsibilities is to move contracts from drafting through execution, and we are frequently asked for an estimate of processing time. The current model used for estimates has a very larger window, taking anywhere between one and twelve weeks.  This results in difficulty accurately forecasting project expenses and adhering to project timelines.  Furthermore, delays in contract execution can result in additional expenses because other parts of a complex project are put on hold to accommodate uncertain start dates.  This can also create variable deliverable quality in outcomes and risks to department goals. 

The number of variables and dependencies involved in the corporate setting results in a expanded project.  There are differing types of contracts, multiple reviewers and stages to the process, widely variable dollar amounts, and differing vendors.  Additionally, each contract is by nature a distinct engagement, and there are external timing factors, such as year-end financial processes or quarterly forecasts, that may influence the timing. 

## Other Approaches

Other approaches to the problem of contract execution timing which have been attempted have not been very successful.  
1. Amount based:  For several years project managers estimated the timing based on the amount of contract, but routinely underestimated the smaller contracts.  

2.  Confidence Interval <provide details here of current process> 
The current model in use examined historical data to determine an 80% confidence interval around the mean for all contracts and provides the upper bound as a pessimistic estimate. For example, all contracts greater than $500K will be executed within 60 business days.  While this model is still valid, this upper bound does not provide the precise timing needed to accurately plan a larger project.  \

## Hypothesis for this analysis 
My hypothesis is that a time-series analysis of two years of historical data, or around 400 records, could improve accuracy of predictions for execution time if all available variables are included. 

By modeling the data as a time-series I hope to account for variability introduced by external cyclical factors.  The inclusion variables such as type of contract, vendor, processors, amounts, and other features will refine the estimation capability. In addition to better execution predictions, any consistent patterns that emerge and analysis of outliers will enable a better understanding of key drivers for delays in execution.

## Project Goals
This project has two main objectives. The first goal is to create a valid predictive model of contract exceution time that can be applied to new submissions.  The second goal is to discover which varibales have the greatest influence on contract timing. Understanding these variables can help make informed business decisions when developing new contract requirments.  


## Data Exploration 
253 Complete Cases
18 variables 

< insert the table here >


## Preliminary Analysis - Supplier Selection

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(DataExplorer) #this one needs unpacked (cb)


#set file name - to be change by github link#
contracts <- "contracts.csv"


# Load Trainning Data Set
dfcontracts<-read.csv(contracts, header=TRUE, sep=",",stringsAsFactors = F)

#convert Dates

#convert to ts  for ts forecasting
#split into training vs. test sets (future looking - model)


#data analysis of the indifvidual variables

#Vendor analysis 
dfv <- dfcontracts%>%select(Vendor.ID, Scaled.Amount)%>%
  mutate(Amt = as.numeric(Scaled.Amount))%>%
  group_by(Vendor.ID)%>%
  summarise(avg=mean(Amt), count = n())%>%
  arrange(-count)

#discuss decision to group supplier ID by top 7, the rest as 'others'
dfoldv <- dfv%>%filter(count >1)
dfnewv<- dfv%>%filter(count <2)

plot(dfv$Vendor.ID, dfv$avg)
# all except singltons

v15<- dfcontracts$Vendor.ID ==15
v1<- dfcontracts$Vendor.ID ==1
v6<- dfcontracts$Vendor.ID ==6
v18<- dfcontracts$Vendor.ID ==18
v23 <- dfcontracts$Vendor.ID ==23
v5<- dfcontracts$Vendor.ID ==5
v26<- dfcontracts$Vendor.ID ==26
v21<- dfcontracts$Vendor.ID ==21
v19<- dfcontracts$Vendor.ID ==19
v20<- dfcontracts$Vendor.ID ==20
v4<- dfcontracts$Vendor.ID ==4
v11 <- dfcontracts$Vendor.ID ==11
v23<- dfcontracts$Vendor.ID ==23
v10 <- dfcontracts$Vendor.ID ==10
v13 <- dfcontracts$Vendor.ID ==13


#list infrequesnt vendors
vnew <- dfcontracts$Vendor.ID %in% dfnewv$Vendor.ID



#discuss why you think it may be a good idea to use all vendors, all users 
#use package dummy_cols  https://cran.r-project.org/web/packages/fastDummies/vignettes/making-dummy-variables.html
#can do granular for intial model, and then summarise for others.  


#do same thing for users?
#prelim analysis - does the user ID matter? look at boxplots of users, avg total days

dfu <- dfcontracts%>%select(User.ID)%>%
  group_by(User.ID)%>%
  summarise(count = n())%>%
  arrange(-count)

hist(dfcontracts$User.ID)

plot(dfu$User.ID, dfu$count)
p

#update data table to include categorical, remove ID's

dfcon <- rbind(dfcontracts,v15,v1,v6,v18,v23,v5,v26)
#

Convert Doc Type

qplot(Area, Total.Days, data = SOW, geom = c("boxplot", "jitter"))

#convert data to correct forms

dfcontracts$SubDate <- as.Date(as.character(dfcontracts$Submitted.Date),format="%m/%d/%Y")
dfcontracts$EnterDate <- as.Date(as.character(dfcontracts$System.Entered.Date),format="%m/%d/%Y")
dfcontracts$ExecuteDate <- as.Date(as.character(dfcontracts$Contract.Execute.Date),format="%m/%d/%Y")


#Convert to ts
dfcontracts$EnterDate <- as.Date(as.character(dfcontracts$System.Entered.Date),format="%m/%d/%Y")

SubDate.ts <- as.ts(x = atm1[, -1], order.by = atm1$DATE)






```

### Supplier

### dates
