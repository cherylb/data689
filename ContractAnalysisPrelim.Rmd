---
title: "Contract Execution"
author: "Cheryl Bowersox"
date: "October 21, 2018"
output: word_document
---
## Introduction

As part of a large IT organization one of my team's responsibilities is to move contracts from drafting through execution, and we are frequently asked for an estimate of processing time. The current model used for estimates has a very larger window, taking anywhere between one and twelve weeks.  This results in difficulty accurately forecasting project expenses and adhering to project timelines.  Furthermore, delays in contract execution can result in additional expenses because other parts of a complex project are put on hold to accommodate uncertain start dates.  This can also create variable deliverable quality in outcomes and risks to department goals. 

The number of variables and dependencies involved in the corporate setting results in a expanded project.  There are differing types of contracts, multiple reviewers and stages to the process, widely variable dollar amounts, and differing vendors.  Additionally, each contract is by nature a distinct engagement, and there are external timing factors, such as year-end financial processes or quarterly forecasts, that may influence the timing. 

## Other Approaches

Other approaches to the problem of contract execution timing which have been attempted have not been very successful.  
1. Amount based:  For several years project managers estimated the timing based on the amount of contract, but routinely underestimated the smaller contracts.  

2.  Confidence Interval <provide details here of current process> 
The current model in use examined historical data to determine an 80% confidence interval around the mean for all contracts and provides the upper bound as a pessimistic estimate. For example, all contracts greater than $500K will be executed within 60 business days.  While this model is still valid, this upper bound does not provide the precise timing needed to accurately plan a larger project.  

## Hypothesis for this analysis 
My hypothesis is that a time-series analysis of two years of historical data, or around 400 records, could improve accuracy of predictions for execution time if all available variables are included. 

By modeling the data as a time-series I hope to account for variability introduced by external cyclical factors.  The inclusion variables such as type of contract, vendor, processors, amounts, and other features will refine the estimation capability. In addition to better execution predictions, any consistent patterns that emerge and analysis of outliers will enable a better understanding of key drivers for delays in execution.

## Project Goals
This project has two main objectives. The first goal is to create a valid predictive model of contract exceution time that can be applied to new submissions.  The second goal is to discover which varibales have the greatest influence on contract timing. Understanding these variables can help make informed business decisions when developing new contract requirments.  


##Data Exploration
### Data Sources
Data for this analysis was gathered from two systems and combined.  The first data source was a online request intake tool. Data such as requestor, vendor, intake date, and responsible analyst was gathered from this system.  The second data source was a corporate contract processing system, that includes information such as contract amount, funding organization, and execution date. 

Data was combined from both system for each contract submitted and executed between 2/2016 - 8/2018.  

### Deidentification
Because the data sources contained confidential business data, th was deidentified to avoid discolosing any propriatiery informaiton.  This inclued mapping suppliers and requestors to an id number, scaling amounts to between 0 and 1,  and simliar operations for other key metrics. 


###Descirption of source data: 

<< Insert summary field table here >>



### Variable analysis 

```{r libraryload, include = FALSE}
library(tidyverse)    #
#library(knitr)        #
#library(VIM)          # correlation
library(caret)        # correlation, model building
library(corrplot)     # Correlation
#library(mice)         # Imputation
#library(MASS)         # BoxCox Transformation
#library("usdm")
library("DataExplorer")
library(forecast)     # alternate transform
#library(ranger)       # Random Forest Model

#library(parallel)     # Parallel processing for model building
#library(doParallel)   # Parallel processing for model building

library(randomForest) # Get Tree, model interpretation


library(dummy) # create dummy variables for predictive modeling

#load data

filename <- "https://raw.githubusercontent.com/cherylb/Data689/master/contracts.csv"

#filename <- "contracts.csv"

# Load Trainning Data Set
contracts.raw <-read.csv(filename, header=TRUE, sep=",",stringsAsFactors = F)


```


## Data Summary

The initial data set includes `r nrow(contracts.raw)` distinct records, with `r ncol(contracts.raw)` possible variables.  

DRAFT NOTE:  The number of variables will not change, however the number of contracts will change as more data will be collected for this analysis over the next 2 weeks. 

The data consists of multiple categorical variables, such as document type, supplier ID, and user ID. 

Because of the large number of suppliers and users,  an intial analysis is done to determine if they can be grouped in any way.  




```{r setup, include=FALSE}
#data conversions - to dates
contracts <- contracts.raw %>% 
  mutate(Sub.Date = 
           as.Date(as.character(contracts.raw$Submitted.Date), 
                   format="%m/%d/%Y")) %>%
  mutate(Enter.Date = 
           as.Date(as.character(contracts.raw$System.Entered.Date),
                   format="%m/%d/%Y"))%>%
  mutate(Exec.Date = 
           as.Date(as.character(contracts.raw$Contract.Execute.Date),
                   format="%m/%d/%Y")) %>%
  mutate(C.Total = replace_na(as.double(Scaled.C.Total),0))%>%
  mutate(Amount = replace_na(as.double(Scaled.Amount),0))%>%
  select(-Submitted.Date, -System.Entered.Date, -Contract.Execute.Date,
         -Scaled.C.Total,-Scaled.Amount)


#A look at categorical variables
  
qplot(Doc.Type, Total.Days, data = contracts, geom = c("boxplot", "jitter"))

#include some discussion here about contract types, different means, importance of data

# look at user ID's
hist(contracts$User.ID, main ="Frequency by User ID")
dfu <- contracts%>%select(User.ID)%>%
  group_by(User.ID)%>%
  summarise(count = n())%>%
  arrange(-count)

newbs <- dfu%>%filter(count < 3)%>%select(User.ID)



#vendors - same thing
hist(contracts$Vendor.ID)  
dfv <- contracts%>%select(Vendor.ID)%>%
  group_by(Vendor.ID)%>%
  summarise(count = n())%>%
  arrange(-count)

#discuss decision to group supplier ID by top 10, the rest as 'others'
dfnewv<- dfv%>%filter(count <2)

hist(contracts$OrgID)
dfo <- contracts%>%select(OrgID)%>%
  group_by(OrgID)%>%
  summarise(count = n())%>%
  arrange(-count)

dfonew <- dfo%>%filter(count <5)

#Discuss decision to group org as freq or number
# <5 = New



#mutate to add new users, new vendors column, group the newbs, and remove the olds
contracts2 <- contracts %>%
  mutate(New.Users= if_else(User.ID %in% newbs$User.ID, 
                            "NewUser", as.character(User.ID)))%>%
  mutate(New.Vendors = if_else(Vendor.ID %in% dfnewv$Vendor.ID,
                               "NewVendor",as.character(Vendor.ID)))%>%
  mutate(New.Org = if_else(OrgID %in% dfonew$OrgID,
                               "RareOrg",as.character(OrgID)))%>%
  select(-User.ID,-Vendor.ID, -OrgID)




#from fastDummmies package create dummy cols for vendors

catcols <- c("Doc.Type", "New.Org", "New.Users","New.Vendors")

contracts3 <- fastDummies::dummy_cols(contracts2, 
                                   select_columns = catcols,
                                   remove_most_frequent_dummy=TRUE)

contracts4 <-contracts3%>%select(-Doc.Type,-New.Users,-New.Vendors,-New.Org)%>%
  arrange(Sub.Date)

ncol(contracts4)

#From DataExplorer

plot_scatterplot(contracts, by = "Total.Days", title = "Plot by Total Days to Execute")

```
## Discussion of grouping for users and vendors
Users who submit less than 3 contracts make up about 50% of the users.  New dummy variable created New.User when the user is less than 3 submissions.  The remaining users will be given dummy variables.
Similarly, there are 9 vendors with only 1 submission, which will be classified as a new vendor. 
There are 17 distinct possible organizations submiting contract requests.  Of those 17, 8 of them have submitted less than 5 requests in the time frame in question, these are are classified together as a 'rare' org.  The remaning organzations are modeled using dummy categorical variables.  

The new data set with categorical variables is a sparse table with `r ncol(contracts3)` columns.  


<insert more discusison of category splits- why did I leave these so broad (complexity of business situations, to determine if they are influential --> may result in later groupings, but first need to determine which are influential and which are not) 


#time series analysis
Clean and transformed data need to plot ts for all stages based on submit date

```{r plotasts, include = FALSE}

library(xts)

#Order by Sub.Date
#ts1 = by submit date, drop ID column, drop enter date, exec date, look at just total.days

#by Sub Date, model total days
days <- contracts4$Sub.Date
cts1 <- contracts4%>%
  select(-ID,-Enter.Date,-Exec.Date,-Sub.Date)

con.ts1a <- xts(cts1, order.by = days)

plot(con.ts1a$Total.Days, main = "Total Days to execute by day")


plot(con.ts1a$Prelim.Days, main = "Prelim Review Days day")


#Split Data
#just for con.ts1a first

n =nrow(con.ts1a) #253

#split data
train1 <- window(con.ts1a,start = 1,end=202)
test1 <- window(con.ts1a, start=203)



#Model for con.ts1: total days by submit date
# checking ACF
 
#testing - for weekly,biweekly lag
lagdays <- c(1,5, 7, 14, 30,60)
lag.plot(con.ts1a$Total.Days, lags = 6, set.lags = lagdays, do.lines = FALSE)

#Acf(x=con.ts1a$Total.Days,plot=TRUE) 

#acf(con.ts1a$Total.Days, na.action = na.pass)


#modeling as a time series: 
#This is throwing an error in the model
#fit1 <- naive(train1, h=70)
#fcst1 <-forecast(test1, h=70)
#accuracy(fit1a1, testa1)
```


### Discussion of TS
For Total Days by submission date (Sub.Date) ther eis no obvious lag presented when evalulated for daily, weekly, and a monthly lag. 
The irregulator nature of the work - some days have multiple entries, some days have no entry. Based on the prelimiary plots there may be no simple signal to extract from the data as a straight time series. 


##Forecasting as a TS
< Problem with the data - have not been able to solve- ts models producing error 'R warning In get(x, envir = ns) : restarting interrupted promise evaluation'  this will need to be re-evaluated

##discussion of overall trends here
- decreased variance over time, decreased mean (?)


#Predictive models using VAR, glm, tree, random forest models


Mid-point analysis: Need further data exploration to be able to create meaninful models, glm wiht 72 variables gives us an RMSE almost 0 and R = 1, which is not correct.   
Linear combination on Prelim + System = Total.Days, need to remove these values



Outstanding  questions: 

Too many dummy variables for glm - can we group more meaninful way? does a tree model work better? can we simplify modeling by excluding user ? or vendor? 
based on SME research - user may be irrlevant if vendor is well known, or analyst = J. 
If user /vendor is removed or binned it reduces # of variables dramatically?

Do distince models for doc types make sense?
Does it make sense to model prelim vs. system seperatley and then combine results?






Is vendor relevant if we have vendor categories (Master/preferred, etc)?



```{r}
#back to our data frame contracts4

#split the data

set.seed(42) 
sample = sample.int(n = nrow(contracts4), size = floor(.80*nrow(contracts)), replace = F)

contrain = contracts4[sample, ]
contest  = contracts4[-sample,]

#VAR model here
#TBD


#To many predictors to run a GLM 

#GLM Modle here
# first, start with a general linear model 
myControl = trainControl(method = 'cv', number = 5, 
  verboseIter = FALSE, savePredictions = TRUE,allowParallel = T)
  
set.seed(143)

con.m1 <- contrain %>%select(-Prelim.Days, -System.Days)
GLM_M1_Data1 = train(Total.Days ~ ., data = con.m1 , metric = 'RMSE', method = 'glm',preProcess = c('center', 'scale'), trControl = myControl)
GLM_M1_Data1



#Nueral Net Model Here


#Random Forest Model Here
